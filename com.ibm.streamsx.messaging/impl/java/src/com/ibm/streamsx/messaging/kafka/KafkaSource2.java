/* Generated by Streams Studio: May 26, 2015 1:20:31 PM EDT */
package com.ibm.streamsx.messaging.kafka;
 

import kafka.api.FetchRequest;
import kafka.api.FetchRequestBuilder;
import kafka.api.PartitionOffsetRequestInfo;
import kafka.common.ErrorMapping;
import kafka.common.TopicAndPartition;
import kafka.javaapi.*;
import kafka.javaapi.consumer.SimpleConsumer;
import kafka.message.MessageAndOffset; 

import java.io.File;
import java.io.FileReader;
import java.io.IOException;
import java.net.URI;
import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.Collections;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Properties;

import org.apache.log4j.Logger;

import scala.actors.threadpool.Arrays;

import com.ibm.streams.operator.AbstractOperator;
import com.ibm.streams.operator.OperatorContext;
import com.ibm.streams.operator.OutputTuple;
import com.ibm.streams.operator.StreamingOutput;
import com.ibm.streams.operator.log4j.TraceLevel;
import com.ibm.streams.operator.model.OutputPortSet;
import com.ibm.streams.operator.model.OutputPortSet.WindowPunctuationOutputMode;
import com.ibm.streams.operator.model.Libraries;
import com.ibm.streams.operator.model.OutputPorts;
import com.ibm.streams.operator.model.Parameter;
import com.ibm.streams.operator.model.PrimitiveOperator;
import com.ibm.streams.operator.state.Checkpoint;
import com.ibm.streams.operator.state.ConsistentRegionContext;
import com.ibm.streams.operator.state.StateHandler;

@Libraries({ "opt/downloaded/*" })
@PrimitiveOperator(name="KafkaSource2", namespace="com.ibm.streamsx.messaging.kafka",
description="Java Operator KafkaSource2")
@OutputPorts({@OutputPortSet(description="Port that produces tuples", cardinality=1, optional=false, windowPunctuationOutputMode=WindowPunctuationOutputMode.Generating), @OutputPortSet(description="Optional output ports", optional=true, windowPunctuationOutputMode=WindowPunctuationOutputMode.Generating)})
public class KafkaSource2 extends AbstractOperator implements StateHandler {

	private static Logger TRACE = Logger.getLogger(KafkaSource2.class.getName());
    private Thread processThread;
	static final String OPER_NAME = "KafkaConsumer";
	private ConsistentRegionContext crContext;
	private boolean shutdown = false;
	private long triggerCount;
	private long triggerIteration = 0;
	//consumer variables
	public SimpleConsumer myConsumer = null;
	String a_topic;
	String leadBroker;
	int a_port;
	long readOffset;
	long resetReadOffset = -1;
	int a_partition;
	int so_timeout=10000;
	String clientName; 
	protected String propertiesFile = null;
	List<String> m_replicaBrokers = new ArrayList<String>();
	protected Properties properties = new Properties(),
			finalProperties = new Properties();
	
	
    @Override
    public synchronized void initialize(OperatorContext context)
			throws Exception {
    	
		super.initialize(context);
        Logger.getLogger(this.getClass()).trace("Operator " + context.getName() + " initializing in PE: " + context.getPE().getPEId() + " in Job: " + context.getPE().getJobId() );
        crContext = context.getOptionalContext(ConsistentRegionContext.class);
        TRACE.log(TraceLevel.TRACE, "Beginning consumer initialization");
        String propFile = getPropertiesFile();
		if (propFile != null) {
			finalProperties.load(new FileReader(propFile));
		}
		finalProperties.putAll(properties);
		if (finalProperties == null || finalProperties.isEmpty())
			throw new Exception(
					"Kafka connection properties must be specified.");
		TRACE.log(TraceLevel.TRACE, "Final properties: " + finalProperties.toString()); 
		
		//name client
		clientName = a_topic + "_partition_" + Integer.toString(a_partition) + "_pe_" + context.getPE().getPEId(); 
		TRACE.log(TraceLevel.TRACE, "Initializing consumer with clientName: " + clientName); 
			
		String brokerString = finalProperties.getProperty("metadata.broker.list");
		List<String> hostAndPortStrings = Arrays.asList(brokerString.split(","));

		PartitionMetadata metadata = findLeader(hostAndPortStrings, a_topic, a_partition);
        if (metadata == null) {
            TRACE.log(TraceLevel.ERROR, "Can't find metadata for Topic and Partition. Exiting.");
            shutdown();
            return;
        }
        if (metadata.leader() == null) {
            TRACE.log(TraceLevel.ERROR, "Can't find Leader for Topic and Partition. Exiting.");
            shutdown();
            return;
        }
		
        TRACE.log(TraceLevel.INFO, "BrokerString: " + brokerString + " Broker List: " + hostAndPortStrings.toString() + " Leader: " + metadata.leader());
		
		leadBroker = metadata.leader().host();
		a_port = metadata.leader().port();
		
		myConsumer = new SimpleConsumer(leadBroker, a_port, so_timeout, 64 * 1024, clientName);
		TRACE.log(TraceLevel.TRACE, "Consumer initialization complete.");
		
		readOffset = getLastOffset(myConsumer, a_topic, a_partition,
				kafka.api.OffsetRequest.LatestTime(), clientName);
		

		/*
         * Create the thread for producing tuples. 
         * The thread is created at initialize time but started.
         * The thread will be started by allPortsReady().
         */
        processThread = getOperatorContext().getThreadFactory().newThread(
                new Runnable() {

                    @Override
                    public void run() {
                        try {
                            produceTuples();
                        } catch (Exception e) {
                            Logger.getLogger(this.getClass()).error("Operator error", e); //$NON-NLS-1$
                        }                    
                    }
                    
                });
        
        /*
         * Set the thread not to be a daemon to ensure that the SPL runtime
         * will wait for the thread to complete before determining the
         * operator is complete.
         */
        processThread.setDaemon(false);
	}
	
	/**
     * Notification that initialization is complete and all input and output ports 
     * are connected and ready to receive and submit tuples.
     * @throws Exception Operator failure, will cause the enclosing PE to terminate.
     */
    @Override
    public synchronized void allPortsReady() throws Exception {
        OperatorContext context = getOperatorContext();
        Logger.getLogger(this.getClass()).trace("Operator " + context.getName() + " all ports are ready in PE: " + context.getPE().getPEId() + " in Job: " + context.getPE().getJobId() );
    	// Start a thread for producing tuples because operator 
    	// implementations must not block and must return control to the caller.
        processThread.start();
    }
    
    
    /**
     * Submit new tuples to the output stream
     * @throws Exception if an error occurs while submitting a tuple
     */
    private void produceTuples() throws Exception  {
        final StreamingOutput<OutputTuple> out = getOutput(0);
       

        OutputTuple tuple = out.newTuple();
		long numRead;
		long currentOffset;
		FetchResponse fetchResponse;

		
		while(!shutdown){
			//TRACE.log(TraceLevel.TRACE, "Iteration through the while loop. ReadOffset: " + Long.toString(readOffset));
			
			if (myConsumer == null){
				myConsumer = new SimpleConsumer(leadBroker, a_port, so_timeout, 64*1024, clientName);	
			}
			
			FetchRequest req = new FetchRequestBuilder()
					.clientId(clientName)
					.addFetch(a_topic, a_partition,
							readOffset, 100000).build();
//			System.out.println("Fetched a request! ReadOffset: " + readOffset);
			//TRACE.log(TraceLevel.TRACE, "FetchRequest:" + req.toString());
			
            try{
            	fetchResponse = myConsumer.fetch(req);
            	//TRACE.log(TraceLevel.TRACE, "Fetched new response:" + fetchResponse.toString());
            } catch(Exception e) {
            	//System.out.println("Catching exception " + e);
            	TRACE.log(TraceLevel.ERROR, "Fetch error. Lead server cannot be contacted. Exception: " + e.getStackTrace());
            	myConsumer.close();
                myConsumer = null;
                fetchResponse = null;
                leadBroker = findNewLeader(leadBroker, a_topic, a_partition);
            }			
			
			

			if (fetchResponse != null) {
				
				if (fetchResponse.hasError()){
				     // Something went wrong!
				     short code = fetchResponse.errorCode(a_topic, a_partition);
				     TRACE.log(TraceLevel.ERROR, "Error fetching data from the Broker:" + leadBroker + " Reason: " + code);

				     if (code == ErrorMapping.OffsetOutOfRangeCode())  {
				         // We asked for an invalid offset. This should never happen.
				         TRACE.log(TraceLevel.ERROR, "Tried to request an invalid offset. Exiting.");
				     }
				     myConsumer.close();
				     myConsumer = null;
				     leadBroker = findNewLeader(leadBroker, a_topic, a_partition);
				} else {
					numRead = 0;
					for (MessageAndOffset messageAndOffset : fetchResponse.messageSet(
							a_topic, a_partition)) {
						// TRACE.log(TraceLevel.INFO,
						// "Looping through messageAndOffset.");
		
						currentOffset = messageAndOffset.offset();
						tuple = out.newTuple();
						if (currentOffset < readOffset) {
							TRACE.log(TraceLevel.ERROR, "Found an old offset: "
									+ currentOffset + " Expecting: " + readOffset);
						}
		
						ByteBuffer messagePayload = messageAndOffset.message()
								.payload();
						byte[] messageBytes = new byte[messagePayload.limit()];
						messagePayload.get(messageBytes);
						String message = new String(messageBytes, "UTF-8");
						ByteBuffer keyPayload = messageAndOffset.message().key();
						byte[] keyBytes = new byte[keyPayload.limit()];
						keyPayload.get(keyBytes);
						String key = new String(keyBytes, "UTF-8");
		
						// Set attributes in tuple:
						tuple.setString("message", message);
						tuple.setString("key", key);
		
						try {
							if (crContext != null) {
								crContext.acquirePermit();
							}
							
							//if there has been a reset, we NEED to get out of this loop and do a new fetch request
							if (resetReadOffset != -1){
								readOffset = resetReadOffset;
								resetReadOffset = -1;
								break;
							}
							
//							System.out.println(
//							"Submitting tuple from offset: " + readOffset + " : " +
//							 message + " key: " + key);
							numRead++;
							// Submit tuple to output stream
							out.submit(tuple);
							readOffset = messageAndOffset.nextOffset();
		
							if (crContext != null && crContext.isTriggerOperator()) {
								triggerIteration++;
								if (triggerIteration >= triggerCount) {
									// TRACE.log(TraceLevel.INFO,
									// "Trigger fired. Making consistent.");
									crContext.makeConsistent();
									triggerIteration = 0;
								}
							} 
						} catch (Exception e) {
							TRACE.log(TraceLevel.ERROR,
									"Unexpected exception: " + e.toString());
						} finally {
							// release permit when done submitting
							if (crContext != null) {
								// TRACE.log(TraceLevel.INFO, "Releasing permit.");
								crContext.releasePermit();
							}
						}
					}
					if (numRead == 0) {
		                try {
		                    Thread.sleep(50);
		                } catch (InterruptedException ie) {
		                	TRACE.log(TraceLevel.ERROR, "Exception while sleeping: " + ie);
		                }
		            }
					
				
			} 
			
			
			}
		}
		if (myConsumer != null) myConsumer.close();
    }

    /**
     * Shutdown this operator, which will interrupt the thread
     * executing the <code>produceTuples()</code> method.
     * @throws Exception Operator failure, will cause the enclosing PE to terminate.
     */
    public synchronized void shutdown() throws Exception {
    	shutdown = true;
        if (processThread != null) {
            processThread.interrupt();
            processThread = null;
        }
        OperatorContext context = getOperatorContext();
        Logger.getLogger(this.getClass()).trace("Operator " + context.getName() + " shutting down in PE: " + context.getPE().getPEId() + " in Job: " + context.getPE().getJobId() );
        
        // Must call super.shutdown()
        super.shutdown();
    }
    
    @Override
    public void close() throws IOException {
    	TRACE.log(TraceLevel.INFO, "StateHandler close");
    }

    @Override
    public void checkpoint(Checkpoint checkpoint) throws Exception {
//    	System.out.println("Checkpoint readOffset " + readOffset);
    	TRACE.log(TraceLevel.INFO, "Checkpoint " + checkpoint.getSequenceId());
    	checkpoint.getOutputStream().writeLong(readOffset);
    }

    @Override
    public void drain() throws Exception {
//    	System.out.println("Drain...");
     TRACE.log(TraceLevel.INFO, "Drain...");
    }
    @Override
    public void reset(Checkpoint checkpoint) throws Exception {
//    	System.out.println("Reset");
    	TRACE.log(TraceLevel.INFO, "Reset to checkpoint " + checkpoint.getSequenceId());
    	resetReadOffset = checkpoint.getInputStream().readLong();
    	
//    	System.out.println("Set readOffset to : " + readOffset);
    }
    @Override
    public void resetToInitialState() throws Exception {
//    	System.out.println("ResetToInitial");
     TRACE.log(TraceLevel.INFO, "Reset to initial state");
    }

    @Override
    public void retireCheckpoint(long id) throws Exception {
//    	System.out.println("Retire checkpoint");
     TRACE.log(TraceLevel.INFO, "Retire checkpoint");
    }
    
	/**
	 * 
	 * @param hostAndPortStrings
	 * @param topic
	 * @param partition
	 * @return
	 */
	private PartitionMetadata findLeader(List<String> hostAndPortStrings,
			String topic, int partition) {
		PartitionMetadata returnMetaData = null;
		loop: for (String seed : hostAndPortStrings) {
			SimpleConsumer consumer = null;
			try {
				URI uri = new URI("my://" + seed); // may throw URISyntaxException
				String host = uri.getHost();
				int port = uri.getPort();
				consumer = new SimpleConsumer(host, port, so_timeout, 64 * 1024,
						"leaderLookup");
				List<String> topics = Collections.singletonList(topic);
				TopicMetadataRequest req = new TopicMetadataRequest(topics);
				kafka.javaapi.TopicMetadataResponse resp = consumer.send(req);

				List<TopicMetadata> metaData = resp.topicsMetadata();
				for (TopicMetadata item : metaData) {
					for (PartitionMetadata part : item.partitionsMetadata()) {
						if (part.partitionId() == partition) {
							returnMetaData = part;
							break loop;
						}
					}
				}
			} catch (Exception e) {
				System.out.println("Error communicating with Broker [" + seed
						+ "] to find Leader for [" + topic + ", "
						+ partition + "] Reason: " + e);
				TRACE.log(TraceLevel.ERROR, "Error communicating with Broker [" + seed
						+ "] to find Leader for [" + topic + ", "
						+ partition + "] Reason: " + e);
			} finally {
				if (consumer != null)
					consumer.close();
			}
		}
		if (returnMetaData != null) {
			m_replicaBrokers.clear();
			for (kafka.cluster.Broker replica : returnMetaData.replicas()) {
				String rep = replica.host() + ":" + replica.port();
				TRACE.log(TraceLevel.TRACE, "Adding replica: " + rep);
				m_replicaBrokers.add(rep);
			}
		}
		return returnMetaData;
	}

	/**
	 * 
	 * @param a_oldLeader
	 * @param topic
	 * @param partition
	 * @return
	 */
	private String findNewLeader(String a_oldLeader, String topic,
			int partition) throws Exception {
		for (int i = 0; i < 3; i++) {
			boolean goToSleep = false;
			PartitionMetadata metadata = findLeader(m_replicaBrokers,
					topic, partition);
			if (metadata == null) {
				goToSleep = true;
			} else if (metadata.leader() == null) {
				goToSleep = true;
			} else if (a_oldLeader.equalsIgnoreCase(metadata.leader().host())
					&& i == 0) {
				// first time through if the leader hasn't changed give
				// ZooKeeper a second to recover
				// second time, assume the broker did recover before failover,
				// or it was a non-Broker issue
				//
				goToSleep = true;
			} else {
				return metadata.leader().host();
			}
			if (goToSleep) {
				try {
					Thread.sleep(1000);
				} catch (InterruptedException ie) {
				}
			}
		}
		System.out
				.println("Unable to find new leader after Broker failure. Exiting");
		TRACE.log(TraceLevel.ERROR, "Can't find Leader for Topic and Partition. Exiting.");
		throw new Exception(
				"Unable to find new leader after Broker failure. Exiting");
	}
    
    public static long getLastOffset(SimpleConsumer consumer, String topic,
			int partition, long whichTime, String clientName) {
		TopicAndPartition topicAndPartition = new TopicAndPartition(topic,
				partition);
		Map<TopicAndPartition, PartitionOffsetRequestInfo> requestInfo = new HashMap<TopicAndPartition, PartitionOffsetRequestInfo>();
		requestInfo.put(topicAndPartition, new PartitionOffsetRequestInfo(
				whichTime, 1));
		kafka.javaapi.OffsetRequest request = new kafka.javaapi.OffsetRequest(
				requestInfo, kafka.api.OffsetRequest.CurrentVersion(),
				clientName);
		OffsetResponse response = consumer.getOffsetsBefore(request);

		if (response.hasError()) {
			TRACE.log(TraceLevel.ERROR, "Error fetching data Offset Data the Broker. Reason: "
					+ response.errorCode(topic, partition));
			return 0;
		}
		long[] offsets = response.offsets(topic, partition);
		TRACE.log(TraceLevel.TRACE, "Retrieving offsets: " + Arrays.toString(offsets));
		return offsets[0]; 
	}
    
    public String getPropertiesFile() {
    	TRACE.log(TraceLevel.TRACE, "Properties file: " + propertiesFile);
    	if (propertiesFile == null) return null;
    	File file = new File(propertiesFile);
		
		// if the properties file is relative, the path is relative to the application directory
		if (!file.isAbsolute())
		{
			propertiesFile = getOperatorContext().getPE().getApplicationDirectory().getAbsolutePath() + "/" +  propertiesFile;
		}
		return propertiesFile;
	}
    
    @Parameter(optional = true, description = "Properties file containing kafka properties.  Properties file is recommended to be stored in the etc directory.  If a relative path is specified, the path is relative to the application directory.")
	public void setPropertiesFile(String value) {
		this.propertiesFile = value;
	}
    
    @Parameter(name="topic", optional=false, 
			description="Topic to be subscribed to.")
	public void setTopic(String value) {
		if(value!=null)
			this.a_topic = value;
	}
    
    @Parameter(name="partition", optional=false, 
			description="Partition to subscribe to.")
	public void setPartition(int value) {
	   	this.a_partition = value;
	}
    
    @Parameter(name="triggerCount", optional=true, 
			description="Number of messages between checkpointing for consistent region. This is only relevant to operator driven checkpointing.")
	public void setTriggerCount(int value) {
	   	this.triggerCount = value;
	}
    
    @Parameter(cardinality = -1, optional = true, description = "Specify a Kafka property \\\"key=value\\\" form. "
			+ "This will override any property specified in the properties file.")
	public void setKafkaProperty(List<String> values) {
		for (String value : values) {
			int idx = value.indexOf("=");
			if (idx == -1)
				throw new IllegalArgumentException("Invalid property: " + value
						+ ", not in the key=value format");
			String name = value.substring(0, idx);
			String v = value.substring(idx + 1, value.length());
			properties.setProperty(name, v);
		}
	}
    
    
}



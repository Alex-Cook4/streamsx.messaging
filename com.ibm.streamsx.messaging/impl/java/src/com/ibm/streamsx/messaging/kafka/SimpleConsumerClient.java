/* Generated by Streams Studio: May 26, 2015 1:20:31 PM EDT */
package com.ibm.streamsx.messaging.kafka;

import kafka.api.FetchRequest;
import kafka.api.FetchRequestBuilder;
import kafka.api.PartitionOffsetRequestInfo;
import kafka.cluster.Broker;
import kafka.common.ErrorMapping;
import kafka.common.TopicAndPartition;
import kafka.javaapi.*;
import kafka.javaapi.consumer.SimpleConsumer;
import kafka.message.MessageAndOffset;
import kafka.utils.ZkUtils;
import kafka.utils.ZKStringSerializer$;
import org.I0Itec.zkclient.ZkClient;


import java.io.IOException;
import java.io.UnsupportedEncodingException;
import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Properties;

import org.apache.log4j.Logger;

import scala.actors.threadpool.Arrays;


import com.ibm.streams.operator.OperatorContext;
import com.ibm.streams.operator.OutputTuple;
import com.ibm.streams.operator.StreamingOutput;
import com.ibm.streams.operator.log4j.TraceLevel;
import com.ibm.streams.operator.state.Checkpoint;
import com.ibm.streams.operator.state.ConsistentRegionContext;
import com.ibm.streams.operator.state.StateHandler;

public class SimpleConsumerClient implements StateHandler {

	private static Logger TRACE = Logger.getLogger(SimpleConsumerClient.class
			.getCanonicalName());
	private Thread processThread;
	static final String OPER_NAME = "KafkaConsumer";
	private ConsistentRegionContext crContext;
	private boolean shutdown = false;
	private long triggerCount;
	private long triggerIteration = 0;
	private OperatorContext operContext;
	// consumer variables
	public SimpleConsumer myConsumer = null;
	ZkClient zkClient;
	String topic;
	Broker leadBroker;
	long readOffset;
	long resetReadOffset = -1;
	int partition;
	int so_timeout;
	String clientName;
	protected Properties finalProperties = new Properties();

	public SimpleConsumerClient(String a_topic, int a_partition,
			AttributeHelper topicAH, AttributeHelper keyAH,
			AttributeHelper messageAH, Properties props, long trigCnt) {
		this.topic = a_topic;
		// this.keyAH = keyAH;
		// this.messageAH = messageAH;
		this.partition = a_partition;
		this.finalProperties = props;
		this.triggerCount = trigCnt;
	}

	public void initialize(OperatorContext context) throws Exception {

		// super.initialize(context);
		Logger.getLogger(this.getClass()).trace(
				"Operator " + context.getName() + " initializing in PE: "
						+ context.getPE().getPEId() + " in Job: "
						+ context.getPE().getJobId());
		operContext = context;
		crContext = context.getOptionalContext(ConsistentRegionContext.class);
		TRACE.log(TraceLevel.TRACE, "Beginning consumer initialization");

		// name client
		clientName = topic + "_partition_" + Integer.toString(partition)
				+ "_pe_" + context.getPE().getPEId();
		TRACE.log(TraceLevel.TRACE, "Initializing consumer with clientName: "
				+ clientName);
		
		//default 30 seconds
		so_timeout = Integer.parseInt(finalProperties.getProperty("socket.timeout.ms", "30000"));
		
		zkClient = getInitializedZkClient();
		leadBroker = findLeader(zkClient, topic, partition);

		if (leadBroker == null) {
			TRACE.log(TraceLevel.ERROR,
					"Can't find Leader for Topic and Partition. Exiting.");
			shutdown();
			return;
		}

		myConsumer = new SimpleConsumer(leadBroker.host(), leadBroker.port(),
				so_timeout, 64 * 1024, clientName);
		TRACE.log(TraceLevel.TRACE, "Consumer initialization complete.");

		readOffset = getLastOffset(myConsumer, topic, partition,
				kafka.api.OffsetRequest.LatestTime(), clientName);

		/*
		 * Create the thread for producing tuples. The thread is created at
		 * initialize time but started. The thread will be started by
		 * allPortsReady().
		 */
		processThread = context.getThreadFactory().newThread(new Runnable() {

			@Override
			public void run() {
				try {
					produceTuples();
				} catch (Exception e) {
					Logger.getLogger(this.getClass())
							.error("Operator error", e); //$NON-NLS-1$
				}
			}

		});

		/*
		 * Set the thread not to be a daemon to ensure that the SPL runtime will
		 * wait for the thread to complete before determining the operator is
		 * complete.
		 */
		processThread.setDaemon(false);
	}

	/**
	 * Notification that initialization is complete and all input and output
	 * ports are connected and ready to receive and submit tuples.
	 * 
	 * @throws Exception
	 *             Operator failure, will cause the enclosing PE to terminate.
	 */
	public synchronized void allPortsReady() throws Exception {
		Logger.getLogger(this.getClass()).trace(
				"Operator " + operContext.getName()
						+ " all ports are ready in PE: "
						+ operContext.getPE().getPEId() + " in Job: "
						+ operContext.getPE().getJobId());
		// Start a thread for producing tuples because operator
		// implementations must not block and must return control to the caller.
		processThread.start();
	}

	/**
	 * Submit new tuples to the output stream
	 * 
	 * @throws Exception
	 *             if an error occurs while submitting a tuple
	 */
	private void produceTuples() throws Exception {
		final StreamingOutput<OutputTuple> out = operContext
				.getStreamingOutputs().get(0);
		OutputTuple tuple = out.newTuple();
		long numRead;
		long currentOffset;
		FetchResponse fetchResponse;

		while (!shutdown) {
			TRACE.log(
					TraceLevel.TRACE,
					"Iteration through the while loop. ReadOffset: "
							+ Long.toString(readOffset));

			if (myConsumer == null) {
				myConsumer = new SimpleConsumer(leadBroker.host(),
						leadBroker.port(), so_timeout, 64 * 1024, clientName);
			}

			FetchRequest req = new FetchRequestBuilder().clientId(clientName)
					.addFetch(topic, partition, readOffset, 100000).build();

			try {
				fetchResponse = myConsumer.fetch(req);
			} catch (Exception e) {
				// System.out.println("Catching exception " + e);
				TRACE.log(TraceLevel.ERROR,
						"Fetch error. Lead server cannot be contacted. Exception: "
								+ e.getStackTrace());
				myConsumer.close();
				myConsumer = null;
				fetchResponse = null;
				leadBroker = findNewLeader(leadBroker, zkClient, topic,
						partition);
			}

			if (fetchResponse != null) {

				if (fetchResponse.hasError()) {
					// Something went wrong!
					handleFetchError(fetchResponse);			
				} else {
					numRead = 0;
					for (MessageAndOffset messageAndOffset : fetchResponse
							.messageSet(topic, partition)) {
						currentOffset = messageAndOffset.offset();
						tuple = out.newTuple();
						
						if (currentOffset < readOffset) {
							TRACE.log(TraceLevel.ERROR, "Found an old offset: "
									+ currentOffset + " Expecting: "
									+ readOffset);
						}
						
						String message = getMessageFromMessageAndOffset(messageAndOffset);
						String key = getKeyFromMessageAndOffset(messageAndOffset);

						// Set attributes in tuple:
						tuple.setString("message", message);
						tuple.setString("key", key);

						try {
							if (crContext != null) {
								crContext.acquirePermit();
							}

							// if there has been a reset, we NEED to get out of
							// this loop and do a new fetch request
							if (resetReadOffset != -1) {
								readOffset = resetReadOffset;
								resetReadOffset = -1;
								break;
							}

							numRead++;
							// Submit tuple to output stream
							out.submit(tuple);
							readOffset = messageAndOffset.nextOffset();

							if (crContext != null
									&& crContext.isTriggerOperator()) {
								triggerIteration++;
								if (triggerIteration >= triggerCount) {

									crContext.makeConsistent();
									triggerIteration = 0;
								}
							}
						} catch (Exception e) {
							TRACE.log(TraceLevel.ERROR,
									"Unexpected exception: " + e.toString());
						} finally {
							// release permit when done submitting
							if (crContext != null) {
								crContext.releasePermit();
							}
						}
					}
					if (numRead == 0) {
						try {
							Thread.sleep(50);
						} catch (InterruptedException ie) {
							TRACE.log(TraceLevel.ERROR,
									"Exception while sleeping: " + ie);
						}
					}

				}

			}
		}
		if (myConsumer != null)
			myConsumer.close();
	}

	private String getKeyFromMessageAndOffset(MessageAndOffset messageAndOffset) throws UnsupportedEncodingException {
		ByteBuffer keyPayload = messageAndOffset.message()
				.key();
		byte[] keyBytes = new byte[keyPayload.limit()];
		keyPayload.get(keyBytes);
		String key = new String(keyBytes, "UTF-8");
		return key;
	}

	private String getMessageFromMessageAndOffset(
			MessageAndOffset messageAndOffset) throws UnsupportedEncodingException {
		
		ByteBuffer messagePayload = messageAndOffset.message()
				.payload();
		
		byte[] messageBytes = new byte[messagePayload.limit()];
		messagePayload.get(messageBytes);
		String message = new String(messageBytes, "UTF-8");
		return message;
	}

	private void handleFetchError(FetchResponse fetchResponse) {
		short code = fetchResponse.errorCode(topic, partition);
		TRACE.log(
				TraceLevel.ERROR,
				"Error fetching data from the Broker:"
						+ leadBroker.host() + " Reason: " + code);

		if (code == ErrorMapping.OffsetOutOfRangeCode()) {
			// We asked for an invalid offset. This should never
			// happen.
			TRACE.log(TraceLevel.ERROR,
					"Tried to request an invalid offset. Exiting.");
		}
		myConsumer.close();
		myConsumer = null;
		leadBroker = findLeader(zkClient, topic, partition);
	}

	/**
	 * Shutdown this operator, which will interrupt the thread executing the
	 * <code>produceTuples()</code> method.
	 * 
	 * @throws Exception
	 *             Operator failure, will cause the enclosing PE to terminate.
	 */
	public synchronized void shutdown() throws Exception {
		shutdown = true;
		if (processThread != null) {
			processThread.interrupt();
			processThread = null;
		}
		// OperatorContext context = getOperatorContext();
		Logger.getLogger(this.getClass()).trace(
				"Operator " + operContext.getName() + " shutting down in PE: "
						+ operContext.getPE().getPEId() + " in Job: "
						+ operContext.getPE().getJobId());

		// Must call super.shutdown()
		// super.shutdown();
	}

	@Override
	public void close() throws IOException {
		TRACE.log(TraceLevel.INFO, "StateHandler close");
	}

	@Override
	public void checkpoint(Checkpoint checkpoint) throws Exception {
		// System.out.println("Checkpoint readOffset " + readOffset);
		TRACE.log(TraceLevel.INFO, "Checkpoint " + checkpoint.getSequenceId());
		checkpoint.getOutputStream().writeLong(readOffset);
	}

	@Override
	public void drain() throws Exception {
		// System.out.println("Drain...");
		TRACE.log(TraceLevel.INFO, "Drain...");
	}

	@Override
	public void reset(Checkpoint checkpoint) throws Exception {
		// System.out.println("Reset");
		TRACE.log(TraceLevel.INFO,
				"Reset to checkpoint " + checkpoint.getSequenceId());
		resetReadOffset = checkpoint.getInputStream().readLong();

		// System.out.println("Set readOffset to : " + readOffset);
	}

	@Override
	public void resetToInitialState() throws Exception {
		// System.out.println("ResetToInitial");
		TRACE.log(TraceLevel.INFO, "Reset to initial state");
	}

	@Override
	public void retireCheckpoint(long id) throws Exception {
		// System.out.println("Retire checkpoint");
		TRACE.log(TraceLevel.INFO, "Retire checkpoint");
	}

	private ZkClient getInitializedZkClient() {
		ZkClient localZkClient;
		String zkConnect = finalProperties.getProperty("zookeeper.connect");
		int zkSessionTimeout = getIntegerProperty(
				"zookeeper.session.timeout.ms", 400);
		int zkConnectionTimeout = getIntegerProperty("zookeeper.sync.time.ms",
				200);

		TRACE.log(TraceLevel.INFO,
				"Initializing ZooKeeper with values: zkConnect(" + zkConnect
						+ ") zkSessionTimeout(" + zkSessionTimeout
						+ ") zkConnectionTimeout(" + zkConnectionTimeout + ")");
		
		try {
			localZkClient = new ZkClient(zkConnect, zkSessionTimeout,
					zkConnectionTimeout, ZKStringSerializer$.MODULE$);
		} catch (Exception e) {
			TRACE.log(TraceLevel.ERROR,
					"Zookeeper client did not initialize correctly with exception: "
							+ e);
			throw e;
		}

		return localZkClient;
	}

	private int getIntegerProperty(String propName, int defaultVal) {
		int integerProp = defaultVal;
		try { 
			integerProp = Integer.parseInt(finalProperties
				.getProperty(propName));
		} catch (Exception e){
			e.printStackTrace();
			TRACE.log(TraceLevel.ERROR, "Property " + propName + "was not input as type int. Exception: " + e);
		}
		
		return integerProp;
	}

	private Broker findLeader(ZkClient a_zkClient, String a_topic,
			int a_partition) {
		int leaderID;
		Broker a_leadBroker = null;

		try {

			leaderID = ZkUtils.getLeaderForPartition(zkClient, a_topic,
					a_partition).get();
			a_leadBroker = ZkUtils.getBrokerInfo(zkClient, leaderID).get();

		} catch (Exception e) {

			e.printStackTrace();
			System.out.println("Exception from ZkClient!");

		}

		return a_leadBroker;
	}

	private Broker findNewLeader(Broker oldBroker, ZkClient a_zkClient,
			String a_topic, int a_partition) {

		for (int i = 0; i < 3; i++) {
			boolean goSleep = false;
			Broker newLeadBroker = findLeader(a_zkClient, a_topic, a_partition);

			if (newLeadBroker == null) {
				goSleep = true;
			} else if (newLeadBroker.host().equalsIgnoreCase(oldBroker.host())
					&& i == 0) {
				// if it's the first time, let's give zookeeper a chance to
				// recover.
				goSleep = true;
			} else {
				return newLeadBroker;
			}

			if (goSleep) {
				try {
					Thread.sleep(1000);
				} catch (InterruptedException ie) {
				}
			}
		}

		return null;
	}

	public static long getLastOffset(SimpleConsumer consumer, String topic,
			int partition, long whichTime, String clientName) {
		TopicAndPartition topicAndPartition = new TopicAndPartition(topic,
				partition);
		Map<TopicAndPartition, PartitionOffsetRequestInfo> requestInfo = new HashMap<TopicAndPartition, PartitionOffsetRequestInfo>();
		requestInfo.put(topicAndPartition, new PartitionOffsetRequestInfo(
				whichTime, 1));
		kafka.javaapi.OffsetRequest request = new kafka.javaapi.OffsetRequest(
				requestInfo, kafka.api.OffsetRequest.CurrentVersion(),
				clientName);
		OffsetResponse response = consumer.getOffsetsBefore(request);

		if (response.hasError()) {
			TRACE.log(TraceLevel.ERROR,
					"Error fetching data Offset Data the Broker. Reason: "
							+ response.errorCode(topic, partition));
			return 0;
		}
		long[] offsets = response.offsets(topic, partition);
		TRACE.log(TraceLevel.TRACE,
				"Retrieving offsets: " + Arrays.toString(offsets));
		return offsets[0];
	}

}

/* Generated by Streams Studio: May 26, 2015 1:20:31 PM EDT */
package com.ibm.streamsx.messaging.kafka;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.File;
import java.io.FileNotFoundException;
import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.io.UnsupportedEncodingException;
import java.nio.ByteBuffer;
import java.util.HashMap;
import java.util.Map;
import java.util.Properties;
 
import kafka.api.FetchRequest;
import kafka.api.FetchRequestBuilder;
import kafka.api.PartitionOffsetRequestInfo;
import kafka.cluster.Broker;
import kafka.common.ErrorMapping;
import kafka.common.TopicAndPartition;
import kafka.javaapi.FetchResponse;
import kafka.javaapi.OffsetResponse;
import kafka.javaapi.consumer.SimpleConsumer;
import kafka.message.MessageAndOffset;
import kafka.utils.ZKStringSerializer$;
import kafka.utils.ZkUtils;

import org.I0Itec.zkclient.ZkClient;
import org.apache.log4j.Logger;

import scala.actors.threadpool.Arrays;

import com.ibm.streams.operator.OperatorContext;
import com.ibm.streams.operator.OutputTuple;
import com.ibm.streams.operator.StreamingOutput;
import com.ibm.streams.operator.log4j.TraceLevel;
import com.ibm.streams.operator.state.Checkpoint;
import com.ibm.streams.operator.state.ConsistentRegionContext;
import com.ibm.streams.operator.state.StateHandler;

public class SimpleConsumerClient implements StateHandler {

	private static Logger TRACE = Logger.getLogger(SimpleConsumerClient.class
			.getCanonicalName());
	
	private Thread processThread;
	static final String OPER_NAME = "KafkaConsumer";
	private ConsistentRegionContext crContext;
	private boolean shutdown = false;
	private long triggerCount;
	private long triggerIteration = 0;
	private OperatorContext operContext;
	private boolean inReset = false;
	// consumer variables
	public SimpleConsumer myConsumer = null;
	private ZkClient zkClient;
	private String topic;
	private Broker leadBroker;
	private long readOffset;
	private int partition;
	private int so_timeout;
	private int bufferSize;
	private int fetchSize;
	private String clientName;
	protected Properties finalProperties = new Properties();
	private String charSet = "UTF-8";
	private AttributeHelper keyAH;
	private AttributeHelper messageAH;
	private int leaderConnectionRetries = 3;
	private int connectionRetryInterval = 1000;
	
	
	private static final int DEFAULT_BUFFER_SIZE = 64 * 1024;
	private static final int DEFAULT_FETCH_SIZE = 100 * 1024;
	private static final int DEFAULT_SO_TIMEOUT = 30 * 1000;

	public SimpleConsumerClient(String a_topic, int a_partition,
			AttributeHelper keyAH,
			AttributeHelper messageAH, Properties props, long trigCnt,
			int connectionRetries, int connectionRetryInterval) {
		this.topic = a_topic;
		this.keyAH = keyAH;
	    this.messageAH = messageAH;
		this.partition = a_partition;
		this.finalProperties = props;
		this.triggerCount = trigCnt;
		this.leaderConnectionRetries = connectionRetries;
		this.connectionRetryInterval = connectionRetryInterval;
	}

	/**
	 * Notification that initialization is complete and all input and output
	 * ports are connected and ready to receive and submit tuples.
	 * 
	 * @throws Exception
	 *             Operator failure, will cause the enclosing PE to terminate.
	 */
	public synchronized void allPortsReady() throws Exception {
		Logger.getLogger(this.getClass()).trace(
				"Operator " + operContext.getName()
						+ " all ports are ready in PE: "
						+ operContext.getPE().getPEId() + " in Job: "
						+ operContext.getPE().getJobId());
		// Start a thread for producing tuples because operator
		// implementations must not block and must return control to the caller.
		processThread.start();
	}

	@Override
	public void checkpoint(Checkpoint checkpoint) throws Exception {
		// System.out.println("Checkpoint readOffset " + readOffset);
		if (TRACE.isInfoEnabled())
			TRACE.log(TraceLevel.INFO, "Checkpoint " + checkpoint.getSequenceId());
		checkpoint.getOutputStream().writeLong(readOffset);
	}

	@Override
	public void close() throws IOException {
		if (TRACE.isInfoEnabled())
			TRACE.log(TraceLevel.INFO, "StateHandler close");
	}

	@Override
	public void drain() throws Exception {
		// System.out.println("Drain...");
		if (TRACE.isInfoEnabled())
			TRACE.log(TraceLevel.INFO, "Drain...");
	}

	private Broker findLeader(ZkClient a_zkClient, String a_topic,
			int a_partition) {
		int leaderID;
		Broker a_leadBroker = null;

		try {

			leaderID = ZkUtils.getLeaderForPartition(zkClient, a_topic,
					a_partition).get();
			a_leadBroker = ZkUtils.getBrokerInfo(zkClient, leaderID).get();

		} catch (Exception e) {

			e.printStackTrace();
			System.out.println("Exception from ZkClient!");
			TRACE.log(TraceLevel.ERROR, "Exception from ZkClient.");

		}

		return a_leadBroker;
	}

	private Broker findNewLeader(Broker oldBroker, ZkClient a_zkClient,
			String a_topic, int a_partition, int numLeaderTries, int sleepBetweenTries) throws IOException{

		for (int i = 0; i < numLeaderTries; i++) {
			boolean goSleep = false;
			Broker newLeadBroker = findLeader(a_zkClient, a_topic, a_partition);

			if (newLeadBroker == null) {
				goSleep = true;
			} else if (oldBroker != null 
					&& newLeadBroker.host().equalsIgnoreCase(oldBroker.host())
					&& i == 0) {
				// if it's the first time, let's give zookeeper a chance to
				// recover.
				goSleep = true;
			} else {
				return newLeadBroker;
			}

			if (goSleep) {
				try {
					if(TRACE.isInfoEnabled())
						TRACE.log(TraceLevel.INFO, "Sleeping after attempt " + (i+1) + ".");
					Thread.sleep(sleepBetweenTries);
				} catch (InterruptedException ie) {
				}
			}
		}
		
		throw new IOException("Unable to find a new Kafka lead Broker after " + numLeaderTries + " attempts. Unable to proceed.");  
	}

	private ZkClient getInitializedZkClient() throws Exception{
		ZkClient localZkClient;
		String zkConnect = finalProperties.getProperty("zookeeper.connect");
		int zkSessionTimeout = getIntegerProperty(
				"zookeeper.session.timeout.ms", 400);
		int zkConnectionTimeout = getIntegerProperty("zookeeper.sync.time.ms",
				200);

		if (TRACE.isInfoEnabled())
			TRACE.log(TraceLevel.INFO,
					"Initializing ZooKeeper with values: zkConnect("
							+ zkConnect + ") zkSessionTimeout("
							+ zkSessionTimeout + ") zkConnectionTimeout("
							+ zkConnectionTimeout + ")");

		try {
			localZkClient = new ZkClient(zkConnect, zkSessionTimeout,
					zkConnectionTimeout, ZKStringSerializer$.MODULE$);
		} catch (Exception e) {
			TRACE.log(TraceLevel.ERROR,
					"Zookeeper client did not initialize correctly with exception: "
							+ e);
			throw e;
		}

		return localZkClient;
	}

	private int getIntegerProperty(String propName, int defaultVal) {
		int integerProp = defaultVal;
		String propVal = finalProperties
		.getProperty(propName); 
		
		if (propVal != null){
			try { 
				integerProp = Integer.parseInt(finalProperties
					.getProperty(propName));
			} catch (Exception e){
				e.printStackTrace();
				TRACE.log(TraceLevel.ERROR, "Property " + propName + " was not input as type int. Exception: " + e);
			}
		}
		
		return integerProp;
	}
	
	public static long getLastOffset(SimpleConsumer consumer, String topic,
			int partition, long whichTime, String clientName) {
		TopicAndPartition topicAndPartition = new TopicAndPartition(topic,
				partition);
		Map<TopicAndPartition, PartitionOffsetRequestInfo> requestInfo = new HashMap<TopicAndPartition, PartitionOffsetRequestInfo>();
		requestInfo.put(topicAndPartition, new PartitionOffsetRequestInfo(
				whichTime, 1));
		kafka.javaapi.OffsetRequest request = new kafka.javaapi.OffsetRequest(
				requestInfo, kafka.api.OffsetRequest.CurrentVersion(),
				clientName);
		OffsetResponse response = consumer.getOffsetsBefore(request);

		if (response.hasError()) {
			TRACE.log(TraceLevel.ERROR,
					"Error fetching data Offset Data the Broker. Reason: "
							+ response.errorCode(topic, partition));
			return 0;
		}
		long[] offsets = response.offsets(topic, partition);
		TRACE.log(TraceLevel.TRACE,
				"Retrieving offsets: " + Arrays.toString(offsets));
		return offsets[0];
	}

	private void handleFetchError(FetchResponse fetchResponse) throws IOException {
		short code = fetchResponse.errorCode(topic, partition);
		TRACE.log(
				TraceLevel.ERROR,
				"Error fetching data from the Broker:"
						+ leadBroker.host() + " Reason: " + code);

		if (code == ErrorMapping.OffsetOutOfRangeCode()) {
			// We asked for an invalid offset. This should never
			// happen.
			TRACE.log(TraceLevel.ERROR,
					"Tried to request an invalid offset. Exiting.");
		}
		myConsumer.close();
		myConsumer = null;
		leadBroker = findNewLeader(null, zkClient, topic, partition, leaderConnectionRetries, connectionRetryInterval);
	}

	public void initialize(OperatorContext context) throws Exception {

		Logger.getLogger(this.getClass()).trace(
				"Operator " + context.getName() + " initializing in PE: "
						+ context.getPE().getPEId() + " in Job: "
						+ context.getPE().getJobId());
		operContext = context;
		crContext = context.getOptionalContext(ConsistentRegionContext.class);
		TRACE.log(TraceLevel.TRACE, "Beginning consumer initialization");

		// name client
		clientName = topic + "_partition_" + Integer.toString(partition)
				+ "_pe_" + context.getPE().getPEId();
		TRACE.log(TraceLevel.TRACE, "Initializing consumer with clientName: "
				+ clientName);
		
		so_timeout = getIntegerProperty("socket.timeout.ms", DEFAULT_SO_TIMEOUT);
		bufferSize = getIntegerProperty("simpleConsumer.bufferSize.bytes", DEFAULT_BUFFER_SIZE);
		fetchSize = getIntegerProperty("simpleConsumer.fetchSize.bytes", DEFAULT_FETCH_SIZE);
		
		
		zkClient = getInitializedZkClient();
		leadBroker = findNewLeader(null, zkClient, topic, partition, leaderConnectionRetries, connectionRetryInterval);

		if (leadBroker == null) {
			TRACE.log(TraceLevel.ERROR,
					"Can't find Leader for Topic and Partition. Exiting.");
			shutdown();
			return;
		}
		
		if (!keyAH.isAvailable()){
			throw new Exception("Specified output attribute " + keyAH.getName() + " is not available.");
		}
		
		if (!messageAH.isAvailable()){
			throw new Exception("Specified output attribute " + messageAH.getName() + " is not available.");
		}
		
		
		myConsumer = new SimpleConsumer(leadBroker.host(), leadBroker.port(),
				so_timeout, bufferSize, clientName);
		if (TRACE.isInfoEnabled())
			TRACE.log(TraceLevel.INFO, "Consumer initialization complete.");
		if (TRACE.isInfoEnabled())
			TRACE.log(TraceLevel.INFO,
					"Initializing SimpleConsumer with values: leadBroker("
							+ leadBroker.host() + ":" + leadBroker.port()
							+ ") socket timeout(" + so_timeout
							+ ") bufferSize(" + bufferSize + ") clientName(" + clientName + ")");

		readOffset = getLastOffset(myConsumer, topic, partition,
				kafka.api.OffsetRequest.LatestTime(), clientName);

//		try (BufferedWriter bw = new BufferedWriter(new FileWriter(new File("/tmp/initialOffset.txt")))){
//			bw.write(Long.toString(readOffset));
//			bw.close();
//		}catch (FileNotFoundException ex) {
//			TRACE.log(TraceLevel.ERROR, "Failed to write out offset!");
//		}
		
		
		/*
		 * Create the thread for producing tuples. The thread is created at
		 * initialize time but started. The thread will be started by
		 * allPortsReady().
		 */
		processThread = context.getThreadFactory().newThread(new Runnable() {

			@Override
			public void run() {
				try {
					produceTuples();
				} catch (Exception e) {
					TRACE.log(TraceLevel.ERROR, "Operator error: " + e.getMessage() + "\n" + e.getStackTrace());
					Logger.getLogger(this.getClass())
							.error("Operator error", e); //$NON-NLS-1$
				}
			}

		});

		/*
		 * Set the thread not to be a daemon to ensure that the SPL runtime will
		 * wait for the thread to complete before determining the operator is
		 * complete.
		 */
		processThread.setDaemon(false);
	}

	/**
	 * Submit new tuples to the output stream
	 * 
	 * @throws Exception
	 *             if an error occurs while submitting a tuple
	 */
	private void produceTuples() throws UnsupportedEncodingException,IOException {
		final StreamingOutput<OutputTuple> out = operContext
				.getStreamingOutputs().get(0);
		OutputTuple tuple = out.newTuple();
		long numRead;
		long currentOffset;
		FetchResponse fetchResponse;

		while (!shutdown) {
			

			if (myConsumer == null) {
				myConsumer = new SimpleConsumer(leadBroker.host(),
						leadBroker.port(), so_timeout, bufferSize, clientName);
			}

			FetchRequest req = new FetchRequestBuilder().clientId(clientName)
					.addFetch(topic, partition, readOffset, fetchSize).build();

			try {
				fetchResponse = myConsumer.fetch(req);
			} catch (Exception e) {
				// System.out.println("Catching exception " + e);
				TRACE.log(TraceLevel.ERROR,
						"Fetch error. Lead server cannot be contacted. Exception: "
								+ e.getStackTrace());
				myConsumer.close();
				myConsumer = null;
				fetchResponse = null;
				leadBroker = findNewLeader(leadBroker, zkClient, topic,
						partition, leaderConnectionRetries, connectionRetryInterval);
			}

			if (fetchResponse != null) {

				if (fetchResponse.hasError()) {
					// Something went wrong!
					handleFetchError(fetchResponse);			
				} else {
					numRead = 0;
					for (MessageAndOffset messageAndOffset : fetchResponse
							.messageSet(topic, partition)) {
						try {
							if (crContext != null) {
								crContext.acquirePermit();
							}
							// if there has been a reset, we NEED to get out of
							// this loop and do a new fetch request
							if (inReset) {
								inReset = false;
								break;
							}
							
							currentOffset = messageAndOffset.offset();
							tuple = out.newTuple();
							
							if (currentOffset < readOffset) {
								TRACE.log(TraceLevel.ERROR, "Found an old offset: "
										+ currentOffset + " Expecting: "
										+ readOffset);
							}
							
							
							ByteBuffer messagePayload = messageAndOffset.message()
									.payload();
							
							byte[] messageBytes = new byte[messagePayload.limit()];
							messagePayload.get(messageBytes);
							String message = new String(messageBytes, charSet);
							
							ByteBuffer keyPayload = messageAndOffset.message()
									.key();
							byte[] keyBytes = new byte[keyPayload.limit()];
							keyPayload.get(keyBytes);
							String key = new String(keyBytes, charSet); 
							
							// Set attributes in tuple:
							tuple.setString(messageAH.getName(), message);
							tuple.setString(keyAH.getName(), key);
						

							

							numRead++;
							// Submit tuple to output stream
							out.submit(tuple);
							readOffset = messageAndOffset.nextOffset();

							if (crContext != null
									&& crContext.isTriggerOperator()) {
								triggerIteration++;
								if (triggerIteration >= triggerCount) {
									crContext.makeConsistent();
									triggerIteration = 0;
								}
							}
						} catch (Exception e) {
							TRACE.log(TraceLevel.ERROR,
									"Unexpected exception: " + e.toString());
						} finally {
							// release permit when done submitting
							if (crContext != null) {
								crContext.releasePermit();
							}
						}
					}
					if (numRead == 0) {
						try {
							Thread.sleep(50);
						} catch (InterruptedException ie) {
							TRACE.log(TraceLevel.ERROR,
									"Exception while sleeping: " + ie);
						}
					}

				}

			}
		}
		if (myConsumer != null)
			myConsumer.close();
	}

	@Override
	public void reset(Checkpoint checkpoint) throws Exception {
		// System.out.println("Reset");
		if (TRACE.isInfoEnabled())
			TRACE.log(TraceLevel.INFO,
					"Reset to checkpoint " + checkpoint.getSequenceId());
		readOffset = checkpoint.getInputStream().readLong();
		inReset = true;
		// System.out.println("Set readOffset to : " + readOffset);
	}

	@Override
	public void resetToInitialState() throws Exception {
		// System.out.println("ResetToInitial");
		if (TRACE.isInfoEnabled())
			TRACE.log(TraceLevel.INFO, "Reset to initial state");
//		try {
//			BufferedReader br = new BufferedReader(new FileReader("/tmp/initialOffset.txt"));
//		    String line = br.readLine();
//		    readOffset = Long.parseLong(line);
//			if (TRACE.isInfoEnabled())
//				TRACE.log(TraceLevel.INFO, "Retrieved initial state from file.");
//		}
	}

	@Override
	public void retireCheckpoint(long id) throws Exception {
		// System.out.println("Retire checkpoint");
		if (TRACE.isInfoEnabled())
			TRACE.log(TraceLevel.INFO, "Retire checkpoint");
	}

	/**
	 * Shutdown this operator, which will interrupt the thread executing the
	 * <code>produceTuples()</code> method.
	 * 
	 * @throws Exception
	 *             Operator failure, will cause the enclosing PE to terminate.
	 */
	public synchronized void shutdown() throws Exception {
		shutdown = true;
		if (processThread != null) {
			processThread.interrupt();
			processThread = null;
		}

		Logger.getLogger(this.getClass()).trace(
				"Operator " + operContext.getName() + " shutting down in PE: "
						+ operContext.getPE().getPEId() + " in Job: "
						+ operContext.getPE().getJobId());

	}

}
